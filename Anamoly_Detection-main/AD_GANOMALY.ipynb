{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9165665,"sourceType":"datasetVersion","datasetId":5538123}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T12:28:52.130982Z","iopub.execute_input":"2024-08-13T12:28:52.131949Z","iopub.status.idle":"2024-08-13T12:28:52.521891Z","shell.execute_reply.started":"2024-08-13T12:28:52.131904Z","shell.execute_reply":"2024-08-13T12:28:52.520783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:28:52.523752Z","iopub.execute_input":"2024-08-13T12:28:52.524210Z","iopub.status.idle":"2024-08-13T12:29:04.377774Z","shell.execute_reply.started":"2024-08-13T12:28:52.524177Z","shell.execute_reply":"2024-08-13T12:29:04.376988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(r\"/kaggle/input/cellular-anomaly/ML-MATT-CompetitionQT1920_train.csv\", encoding=\"latin-1\")\ntest_data = pd.read_csv(r\"/kaggle/input/cellular-anomaly/ML-MATT-CompetitionQT1920_test.csv\", encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.378898Z","iopub.execute_input":"2024-08-13T12:29:04.379438Z","iopub.status.idle":"2024-08-13T12:29:04.528006Z","shell.execute_reply.started":"2024-08-13T12:29:04.379411Z","shell.execute_reply":"2024-08-13T12:29:04.527135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.530012Z","iopub.execute_input":"2024-08-13T12:29:04.530322Z","iopub.status.idle":"2024-08-13T12:29:04.569338Z","shell.execute_reply.started":"2024-08-13T12:29:04.530292Z","shell.execute_reply":"2024-08-13T12:29:04.568244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.570457Z","iopub.execute_input":"2024-08-13T12:29:04.570784Z","iopub.status.idle":"2024-08-13T12:29:04.602005Z","shell.execute_reply.started":"2024-08-13T12:29:04.570760Z","shell.execute_reply":"2024-08-13T12:29:04.600993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data['maxUE_UL+DL'].value_counts().get('#¡VALOR!', 0))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.603105Z","iopub.execute_input":"2024-08-13T12:29:04.603393Z","iopub.status.idle":"2024-08-13T12:29:04.613445Z","shell.execute_reply.started":"2024-08-13T12:29:04.603369Z","shell.execute_reply":"2024-08-13T12:29:04.612508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace '#¡VALOR!' with NaN\ntrain_data['maxUE_UL+DL'] = train_data['maxUE_UL+DL'].replace('#¡VALOR!', np.nan)\n\n# Convert to numeric, coercing any remaining non-numeric values to NaN\ntrain_data['maxUE_UL+DL'] = pd.to_numeric(train_data['maxUE_UL+DL'], errors='coerce')\n\n# Check the number of NaN values\nprint(train_data['maxUE_UL+DL'].isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.614601Z","iopub.execute_input":"2024-08-13T12:29:04.614879Z","iopub.status.idle":"2024-08-13T12:29:04.661651Z","shell.execute_reply.started":"2024-08-13T12:29:04.614858Z","shell.execute_reply":"2024-08-13T12:29:04.660786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"median_value = train_data['maxUE_UL+DL'].median()\ntrain_data['maxUE_UL+DL'] = train_data['maxUE_UL+DL'].fillna(median_value)\n\n# Convert to int64\ntrain_data['maxUE_UL+DL'] = train_data['maxUE_UL+DL'].astype('int64')\n\n# Verify the change\nprint(train_data['maxUE_UL+DL'].dtype)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.662638Z","iopub.execute_input":"2024-08-13T12:29:04.662890Z","iopub.status.idle":"2024-08-13T12:29:04.671639Z","shell.execute_reply.started":"2024-08-13T12:29:04.662868Z","shell.execute_reply":"2024-08-13T12:29:04.670675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.672966Z","iopub.execute_input":"2024-08-13T12:29:04.673256Z","iopub.status.idle":"2024-08-13T12:29:04.686203Z","shell.execute_reply.started":"2024-08-13T12:29:04.673233Z","shell.execute_reply":"2024-08-13T12:29:04.685178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.689384Z","iopub.execute_input":"2024-08-13T12:29:04.689672Z","iopub.status.idle":"2024-08-13T12:29:04.702752Z","shell.execute_reply.started":"2024-08-13T12:29:04.689649Z","shell.execute_reply":"2024-08-13T12:29:04.701698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.703850Z","iopub.execute_input":"2024-08-13T12:29:04.704153Z","iopub.status.idle":"2024-08-13T12:29:04.711387Z","shell.execute_reply.started":"2024-08-13T12:29:04.704130Z","shell.execute_reply":"2024-08-13T12:29:04.710504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_columns = train_data.select_dtypes(include=['float64', 'int64']).columns\n\nnumerical_columns","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.712401Z","iopub.execute_input":"2024-08-13T12:29:04.712692Z","iopub.status.idle":"2024-08-13T12:29:04.721464Z","shell.execute_reply.started":"2024-08-13T12:29:04.712661Z","shell.execute_reply":"2024-08-13T12:29:04.720475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify numerical columns (excluding 'Unusual' from train_data)\ntrain_numerical_columns = train_data.select_dtypes(include=['float64', 'int64']).columns.drop('Unusual')\ntest_numerical_columns = test_data.select_dtypes(include=['float64', 'int64']).columns\n\n# Fill missing values with median for numerical columns\ntrain_data[train_numerical_columns] = train_data[train_numerical_columns].fillna(train_data[train_numerical_columns].median())\ntest_data[test_numerical_columns] = test_data[test_numerical_columns].fillna(test_data[test_numerical_columns].median())","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.722795Z","iopub.execute_input":"2024-08-13T12:29:04.723392Z","iopub.status.idle":"2024-08-13T12:29:04.759148Z","shell.execute_reply.started":"2024-08-13T12:29:04.723359Z","shell.execute_reply":"2024-08-13T12:29:04.758429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode 'CellName'\nle = LabelEncoder()\ntrain_data['CellName_encoded'] = le.fit_transform(train_data['CellName'])\ntest_data['CellName_encoded'] = le.transform(test_data['CellName'])\n\n# Convert 'Time' to datetime and extract features\ntrain_data['Time'] = pd.to_datetime(train_data['Time'], format='%H:%M')\ntest_data['Time'] = pd.to_datetime(test_data['Time'], format='%H:%M')\n\ntrain_data['Hour'] = train_data['Time'].dt.hour\ntrain_data['DayOfWeek'] = train_data['Time'].dt.dayofweek\ntest_data['Hour'] = test_data['Time'].dt.hour\ntest_data['DayOfWeek'] = test_data['Time'].dt.dayofweek","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.760201Z","iopub.execute_input":"2024-08-13T12:29:04.760556Z","iopub.status.idle":"2024-08-13T12:29:04.858055Z","shell.execute_reply.started":"2024-08-13T12:29:04.760520Z","shell.execute_reply":"2024-08-13T12:29:04.857083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_data[train_numerical_columns] = scaler.fit_transform(train_data[train_numerical_columns])\ntest_data[test_numerical_columns] = scaler.transform(test_data[test_numerical_columns])","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.859256Z","iopub.execute_input":"2024-08-13T12:29:04.859563Z","iopub.status.idle":"2024-08-13T12:29:04.881647Z","shell.execute_reply.started":"2024-08-13T12:29:04.859539Z","shell.execute_reply":"2024-08-13T12:29:04.880939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = train_numerical_columns.tolist() + ['CellName_encoded', 'Hour', 'DayOfWeek']\nX_train = train_data[features]\ny_train = train_data['Unusual']\nX_test = test_data[features]","metadata":{"execution":{"iopub.status.busy":"2024-08-13T12:29:04.882559Z","iopub.execute_input":"2024-08-13T12:29:04.882817Z","iopub.status.idle":"2024-08-13T12:29:04.893242Z","shell.execute_reply.started":"2024-08-13T12:29:04.882794Z","shell.execute_reply":"2024-08-13T12:29:04.892483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#usual_count\ntest_data[test_data['Anomaly']==0]['Anomaly'].count()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T13:01:28.763550Z","iopub.execute_input":"2024-08-13T13:01:28.764255Z","iopub.status.idle":"2024-08-13T13:01:28.773335Z","shell.execute_reply.started":"2024-08-13T13:01:28.764226Z","shell.execute_reply":"2024-08-13T13:01:28.772341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.FloatTensor(X_train.values)\nX_test_tensor = torch.FloatTensor(X_test.values)\n\n# Create DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass GANomaly(nn.Module):\n    def _init_(self, input_dim):\n        super(GANomaly, self)._init_()\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Linear(16, 8)\n        )\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(8, 16),\n            nn.ReLU(),\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Linear(64, input_dim)\n        )\n        \n        self.discriminator = nn.Sequential(\n            nn.Linear(input_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Linear(16, 1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        y = self.discriminator(x_hat)\n        return x_hat, y, z\n\n# Initialize the model\ninput_dim = X_train.shape[1]\nmodel = GANomaly(input_dim)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define loss functions and optimizer\ncriterion_mse = nn.MSELoss()\ncriterion_bce = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    for batch in train_loader:\n        inputs = batch[0]\n        \n        # Forward pass\n        x_hat, y, z = model(inputs)\n        \n        # Compute losses\n        loss_con = criterion_mse(inputs, x_hat)\n        loss_enc = criterion_mse(z, model.encoder(x_hat))\n        loss_adv = criterion_bce(y, torch.ones(y.size(0), 1))\n        \n        loss = loss_con + loss_enc + loss_adv\n        \n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nmodel.eval()\nanomaly_scores = []\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = batch[0]\n        x_hat, _, _ = model(inputs)\n        anomaly_score = torch.mean((inputs - x_hat)**2, dim=1)\n        anomaly_scores.extend(anomaly_score.numpy())\n\n# Convert anomaly scores to probabilities\nanomaly_probs = 1 - np.exp(-np.array(anomaly_scores))\n\n# Add predictions to the test data\ntest_data['GANomaly_Score'] = anomaly_scores\ntest_data['GANomaly_Probability'] = anomaly_probs\n\n\n# Display top anomalies\ntop_anomalies = test_data.sort_values('GANomaly_Probability', ascending=False).head(10)\n\n# Select columns to display\ndisplay_columns = ['Time', 'CellName', 'GANomaly_Probability'] + test_numerical_columns.tolist()\n\n# Function to format float values\ndef format_float(x):\n    return f\"{x:.4f}\" if isinstance(x, (float, np.float64)) else x\n\n# Display top anomalies with formatted values\nprint(top_anomalies[display_columns].applymap(format_float))\n\n# Calculate and display average values for comparison\navg_values = test_data[test_numerical_columns].mean()\nprint(\"\\nAverage values for comparison:\")\nprint(avg_values.apply(format_float))","metadata":{},"execution_count":null,"outputs":[]}]}